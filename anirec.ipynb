{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anime Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import \tWordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mal-anime.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = df['Genres']\n",
    "split_genres = genres.str.split(',').apply(lambda x: [i.strip() for i in x])\n",
    "\n",
    "# Flatten the list of genres and find the unique genres\n",
    "unique_genres = pd.unique([item for sublist in split_genres for item in sublist])\n",
    "print(unique_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each unique genre, create a new column in the dataframe. \n",
    "# If the genre is in the anime's genre list, set the value to 1, otherwise 0.\n",
    "for genre in unique_genres:\n",
    "    df[genre] = split_genres.apply(lambda x: 1 if genre in x else 0)\n",
    "df.rename(columns={'Unknown': 'Unknown_Genre'}, inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df[\"Episodes\"] == \"Unknown\"][\"Name\"]))\n",
    "print(df[df[\"Episodes\"] == \"Unknown\"][\"Name\"].head(10))\n",
    "# Drop all rows with 'Unknown' in the 'Episodes' column \n",
    "df = df[df['Episodes'] != 'Unknown']\n",
    "print(len(df[df[\"Score\"] == \"Unknown\"][\"Name\"]))\n",
    "print(df[df[\"Score\"] == \"Unknown\"][\"Name\"].head(10))\n",
    "# Drop all rows with 'Unknown' in the 'Score', 'Ranked', and 'Aired' column \n",
    "df = df[df['Score'] != 'Unknown']\n",
    "df = df[df['Ranked'] != 'Unknown']\n",
    "df = df[df['Aired'] != 'Unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallest_years = []\n",
    "\n",
    "for aired in df['Aired']:\n",
    "    # Extract all years using regular expressions\n",
    "    years = re.findall('(\\d{4})', aired)\n",
    "    \n",
    "    # Convert the years to integers\n",
    "    years = [int(year) for year in years] \n",
    "    if years:\n",
    "        smallest_years.append(min(years))\n",
    "    else:\n",
    "        smallest_years.append(np.nan)\n",
    "\n",
    "# Replace the 'Aired' column with the smallest years\n",
    "df['Aired'] = smallest_years\n",
    "df['Aired'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_seconds(duration):\n",
    "    time_units = duration.split()\n",
    "    seconds = 0\n",
    "    for i in range(0, len(time_units), 2):\n",
    "        if i+1 < len(time_units):\n",
    "            if 'h' in time_units[i+1]:\n",
    "                seconds += int(time_units[i]) * 3600\n",
    "            elif 'm' in time_units[i+1]:\n",
    "                seconds += int(time_units[i]) * 60\n",
    "            elif 's' in time_units[i+1]:\n",
    "                seconds += int(time_units[i])\n",
    "    return seconds\n",
    "\n",
    "df['Duration'] = df['Duration'].apply(convert_to_seconds)\n",
    "df['Duration'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing data types\n",
    "df['Score'] = df['Score'].astype(float)\n",
    "df['Episodes'] = df['Episodes'].astype(np.int64)\n",
    "df['Popularity'] = df['Popularity'].astype(np.int64)\n",
    "df['Members'] = df['Members'].astype(np.int64)\n",
    "df['Completed'] = df['Completed'].astype(np.int64)\n",
    "df['Dropped'] = df['Dropped'].astype(np.int64)\n",
    "df['Ranked'] = df['Ranked'].astype(float)\n",
    "df['Plan to Watch'] = df['Plan to Watch'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN values from 'Duration' column\n",
    "duration = df['Duration'].dropna()\n",
    "aired = df['Aired'].dropna()\n",
    "episodes = df['Episodes'].dropna()\n",
    "\n",
    "# Create a histogram with a density curve\n",
    "plt.hist(duration, bins=30, density=True, alpha=0.6, color='g')\n",
    "\n",
    "plt.title('Distribution of Duration')\n",
    "plt.xlabel('Duration')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Repeat for 'Aired' and 'Episodes'\n",
    "plt.hist(aired, bins=30, density=True, alpha=0.6, color='g')\n",
    "plt.title('Distribution of Aired')\n",
    "plt.xlabel('Aired')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(episodes, bins=30, density=True, alpha=0.6, color='g')\n",
    "plt.title('Distribution of Episodes')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the columns that are not required for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Type', 'Rating', 'Genres', 'Score-1', 'Score-2', 'Score-3', 'Score-4', 'Score-5', 'Score-6', 'Score-7', 'Score-8', 'Score-9', 'Score-10', \n",
    "                   'Ranked', 'Favorites', 'Members' , 'Watching' , 'On-Hold' , 'Completed', 'Dropped', 'Plan to Watch', \n",
    "                   'MAL_ID', 'Name', 'English name' , 'Japanese name' , 'Premiered' , 'Producers', 'Licensors', \n",
    "                   'Source', 'Studios', \n",
    "                   'Unknown_Genre','Hentai', 'Yuri', 'Yaoi', 'Duration']\n",
    "\n",
    "df = df.drop(columns_to_drop, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
